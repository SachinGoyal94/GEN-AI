{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Building Gen Ai apps with llm's\n",
    "LLm's are trained on a data for ex The training data for GPT-4o is limited to data from October 2023 or earlier.\n",
    "but to get info after 2023 with GPT 4-o requires tools with which our LLM interacts\n"
   ],
   "id": "d17251c3a26e2f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:05.232456Z",
     "start_time": "2025-07-09T15:11:04.080081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "#Arxiv-Research Paper\n",
    "#Tools creation\n",
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper,WikipediaAPIWrapper\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from tenacity import retry_if_exception_message"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:05.465815Z",
     "start_time": "2025-07-09T15:11:05.250737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wiki_api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "wiki_tool=WikipediaQueryRun(api_wrapper=wiki_api_wrapper)\n",
    "wiki_tool.name"
   ],
   "id": "a0caaef6e8f0d9b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:06.283650Z",
     "start_time": "2025-07-09T15:11:06.201093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "arxiv_api_wrapper=ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "arxiv_tool=ArxivQueryRun(api_wrapper=arxiv_api_wrapper)\n",
    "arxiv_tool.name"
   ],
   "id": "590b6eab2f0162db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:06.432128Z",
     "start_time": "2025-07-09T15:11:06.428790Z"
    }
   },
   "cell_type": "code",
   "source": "tools=[wiki_tool,arxiv_tool]",
   "id": "34df45a1ffdcf236",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:07.650516Z",
     "start_time": "2025-07-09T15:11:06.535814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Custom Tool (RAG Tool)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings,ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GEMINI_KEY\")"
   ],
   "id": "5c7a43318ad252a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:10.549937Z",
     "start_time": "2025-07-09T15:11:07.663637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs=WebBaseLoader(\"https://docs.smith.langchain.com/\").load()\n",
    "splitted=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50).split_documents(docs)\n",
    "db=FAISS.from_documents(splitted,GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\"))\n",
    "retriever=db.as_retriever()\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ],
   "id": "c13b85d61f019b13",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:10.582542Z",
     "start_time": "2025-07-09T15:11:10.567072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(retriever,name=\"langsmith_search\",description=\"search any information on langsmith\")\n",
    "retriever_tool"
   ],
   "id": "e78b32d12208751b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='langsmith_search', description='search any information on langsmith', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001FEFE94D440>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FEFE923CB0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001FEFE94D580>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FEFE923CB0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:10.676168Z",
     "start_time": "2025-07-09T15:11:10.670542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tools=[arxiv_tool,wiki_tool,retriever_tool]\n",
    "tools"
   ],
   "id": "8ff4db4562951d3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'D:\\\\GEN-AI\\\\.venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " Tool(name='langsmith_search', description='search any information on langsmith', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001FEFE94D440>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FEFE923CB0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001FEFE94D580>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FEFE923CB0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "hub.pull = “get a reusable chain/prompt/agent template from the Hub ”.\n",
    "\n",
    "it's like github where we can use someone's else code so we don't have to makesomething from scratch\n",
    "\n",
    " Inside, it’s basically:\n",
    "\n",
    "A prompt template that tells your LLM how to act like an agent that can call functions/tools.\n",
    "\n",
    "It includes instructions, formatting, and placeholders for:\n",
    "\n",
    "The system message (“You are a helpful assistant that can use tools…”)\n",
    "\n",
    "The user message (what the user asked)\n",
    "\n",
    "Function call JSON structure (so the LLM knows how to format the function/tool call)"
   ],
   "id": "bf5a980ce0740aad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:11.806053Z",
     "start_time": "2025-07-09T15:11:10.754742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Prompt Template\n",
    "from langchain import hub\n",
    "prompt=hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ],
   "id": "9e422e0dead4e7b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "now for llm to use tool we need agents",
   "id": "20e5460081331ded"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:12.301710Z",
     "start_time": "2025-07-09T15:11:11.829664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Agent\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,prompt)\n",
    "agent"
   ],
   "id": "de33bb26ce81cd66",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001FEFBA579C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001FEFBA579C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001FEFE932490>, default_metadata=(), model_kwargs={}), kwargs={'tools': [{'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith_search', 'description': 'search any information on langsmith', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:12.329102Z",
     "start_time": "2025-07-09T15:11:12.320986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Agent Executor\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)#to see the working verbose=True\n",
    "agent_executor"
   ],
   "id": "36ab72c388da8d35",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001FEFBA579C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001FEFBA579C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001FEFE932490>, default_metadata=(), model_kwargs={}), kwargs={'tools': [{'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith_search', 'description': 'search any information on langsmith', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)), WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'D:\\\\GEN-AI\\\\.venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)), Tool(name='langsmith_search', description='search any information on langsmith', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001FEFE94D440>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FEFE923CB0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001FEFE94D580>, retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FEFE923CB0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:17.988666Z",
     "start_time": "2025-07-09T15:11:12.399653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent_executor.invoke({\"input\":\"tell me something about Sachin Tendulkar\"})\n",
    "##this invokes 3 agent executor chain because we gave 3 tools wikipedia arxiv and retriever (for langsmith website)"
   ],
   "id": "a046cf854806b5d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 46\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 44\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mResourceExhausted\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43magent_executor\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minput\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtell me something about Sachin Tendulkar\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m##this invokes 3 agent executor chain because we gave 3 tools wikipedia arxiv and retriever (for langsmith website)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    166\u001B[39m     run_manager.on_chain_error(e)\n\u001B[32m--> \u001B[39m\u001B[32m167\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m    168\u001B[39m run_manager.on_chain_end(outputs)\n\u001B[32m    170\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:157\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    155\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    156\u001B[39m     outputs = (\n\u001B[32m--> \u001B[39m\u001B[32m157\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    158\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    159\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    160\u001B[39m     )\n\u001B[32m    162\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28mself\u001B[39m.prep_outputs(\n\u001B[32m    163\u001B[39m         inputs, outputs, return_only_outputs\n\u001B[32m    164\u001B[39m     )\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1620\u001B[39m, in \u001B[36mAgentExecutor._call\u001B[39m\u001B[34m(self, inputs, run_manager)\u001B[39m\n\u001B[32m   1618\u001B[39m \u001B[38;5;66;03m# We now enter the agent loop (until it returns something).\u001B[39;00m\n\u001B[32m   1619\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_continue(iterations, time_elapsed):\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m     next_step_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_take_next_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1621\u001B[39m \u001B[43m        \u001B[49m\u001B[43mname_to_tool_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1622\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcolor_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1623\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1624\u001B[39m \u001B[43m        \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1625\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1626\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1627\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(next_step_output, AgentFinish):\n\u001B[32m   1628\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._return(\n\u001B[32m   1629\u001B[39m             next_step_output, intermediate_steps, run_manager=run_manager\n\u001B[32m   1630\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1328\u001B[39m, in \u001B[36mAgentExecutor._take_next_step\u001B[39m\u001B[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[39m\n\u001B[32m   1317\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_take_next_step\u001B[39m(\n\u001B[32m   1318\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1319\u001B[39m     name_to_tool_map: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, BaseTool],\n\u001B[32m   (...)\u001B[39m\u001B[32m   1323\u001B[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1324\u001B[39m ) -> Union[AgentFinish, \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mtuple\u001B[39m[AgentAction, \u001B[38;5;28mstr\u001B[39m]]]:\n\u001B[32m   1325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._consume_next_step(\n\u001B[32m   1326\u001B[39m         \u001B[43m[\u001B[49m\n\u001B[32m   1327\u001B[39m \u001B[43m            \u001B[49m\u001B[43ma\u001B[49m\n\u001B[32m-> \u001B[39m\u001B[32m1328\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_iter_next_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1329\u001B[39m \u001B[43m                \u001B[49m\u001B[43mname_to_tool_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1330\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcolor_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1331\u001B[39m \u001B[43m                \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1332\u001B[39m \u001B[43m                \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1333\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1334\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1335\u001B[39m \u001B[43m        \u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m   1336\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1354\u001B[39m, in \u001B[36mAgentExecutor._iter_next_step\u001B[39m\u001B[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[39m\n\u001B[32m   1351\u001B[39m     intermediate_steps = \u001B[38;5;28mself\u001B[39m._prepare_intermediate_steps(intermediate_steps)\n\u001B[32m   1353\u001B[39m     \u001B[38;5;66;03m# Call the LLM to see what to do.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1354\u001B[39m     output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_action_agent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mplan\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1355\u001B[39m \u001B[43m        \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1356\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1357\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1358\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1359\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m OutputParserException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1360\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.handle_parsing_errors, \u001B[38;5;28mbool\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain\\agents\\agent.py:577\u001B[39m, in \u001B[36mRunnableMultiActionAgent.plan\u001B[39m\u001B[34m(self, intermediate_steps, callbacks, **kwargs)\u001B[39m\n\u001B[32m    569\u001B[39m final_output: Any = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    570\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stream_runnable:\n\u001B[32m    571\u001B[39m     \u001B[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001B[39;00m\n\u001B[32m    572\u001B[39m     \u001B[38;5;66;03m# streaming\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    575\u001B[39m     \u001B[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001B[39;00m\n\u001B[32m    576\u001B[39m     \u001B[38;5;66;03m# accumulate the output into final output and return that.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m577\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrunnable\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    578\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfinal_output\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m:\u001B[49m\n\u001B[32m    579\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfinal_output\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3438\u001B[39m, in \u001B[36mRunnableSequence.stream\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   3431\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   3432\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstream\u001B[39m(\n\u001B[32m   3433\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3436\u001B[39m     **kwargs: Optional[Any],\n\u001B[32m   3437\u001B[39m ) -> Iterator[Output]:\n\u001B[32m-> \u001B[39m\u001B[32m3438\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform(\u001B[38;5;28miter\u001B[39m([\u001B[38;5;28minput\u001B[39m]), config, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3424\u001B[39m, in \u001B[36mRunnableSequence.transform\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   3417\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   3418\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtransform\u001B[39m(\n\u001B[32m   3419\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3422\u001B[39m     **kwargs: Optional[Any],\n\u001B[32m   3423\u001B[39m ) -> Iterator[Output]:\n\u001B[32m-> \u001B[39m\u001B[32m3424\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._transform_stream_with_config(\n\u001B[32m   3425\u001B[39m         \u001B[38;5;28minput\u001B[39m,\n\u001B[32m   3426\u001B[39m         \u001B[38;5;28mself\u001B[39m._transform,\n\u001B[32m   3427\u001B[39m         patch_config(config, run_name=(config \u001B[38;5;129;01mor\u001B[39;00m {}).get(\u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.name),\n\u001B[32m   3428\u001B[39m         **kwargs,\n\u001B[32m   3429\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2215\u001B[39m, in \u001B[36mRunnable._transform_stream_with_config\u001B[39m\u001B[34m(self, inputs, transformer, config, run_type, **kwargs)\u001B[39m\n\u001B[32m   2213\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   2214\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2215\u001B[39m         chunk: Output = \u001B[43mcontext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2216\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m chunk\n\u001B[32m   2217\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m final_output_supported:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3386\u001B[39m, in \u001B[36mRunnableSequence._transform\u001B[39m\u001B[34m(self, inputs, run_manager, config, **kwargs)\u001B[39m\n\u001B[32m   3383\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3384\u001B[39m         final_pipeline = step.transform(final_pipeline, config)\n\u001B[32m-> \u001B[39m\u001B[32m3386\u001B[39m \u001B[38;5;28;01myield from\u001B[39;00m final_pipeline\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1429\u001B[39m, in \u001B[36mRunnable.transform\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   1426\u001B[39m final: Input\n\u001B[32m   1427\u001B[39m got_first_val = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1429\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43michunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m:\u001B[49m\n\u001B[32m   1430\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# The default implementation of transform is to buffer input and\u001B[39;49;00m\n\u001B[32m   1431\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# then call stream.\u001B[39;49;00m\n\u001B[32m   1432\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# It'll attempt to gather all input into a single chunk using\u001B[39;49;00m\n\u001B[32m   1433\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# the `+` operator.\u001B[39;49;00m\n\u001B[32m   1434\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If the input is not addable, then we'll assume that we can\u001B[39;49;00m\n\u001B[32m   1435\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# only operate on the last chunk,\u001B[39;49;00m\n\u001B[32m   1436\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# and we'll iterate until we get to the last chunk.\u001B[39;49;00m\n\u001B[32m   1437\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mgot_first_val\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1438\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfinal\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43michunk\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5647\u001B[39m, in \u001B[36mRunnableBindingBase.transform\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   5640\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   5641\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtransform\u001B[39m(\n\u001B[32m   5642\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   5645\u001B[39m     **kwargs: Any,\n\u001B[32m   5646\u001B[39m ) -> Iterator[Output]:\n\u001B[32m-> \u001B[39m\u001B[32m5647\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.bound.transform(\n\u001B[32m   5648\u001B[39m         \u001B[38;5;28minput\u001B[39m,\n\u001B[32m   5649\u001B[39m         \u001B[38;5;28mself\u001B[39m._merge_configs(config),\n\u001B[32m   5650\u001B[39m         **{**\u001B[38;5;28mself\u001B[39m.kwargs, **kwargs},\n\u001B[32m   5651\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1447\u001B[39m, in \u001B[36mRunnable.transform\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   1444\u001B[39m             final = ichunk\n\u001B[32m   1446\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m got_first_val:\n\u001B[32m-> \u001B[39m\u001B[32m1447\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stream(final, config, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:499\u001B[39m, in \u001B[36mBaseChatModel.stream\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    497\u001B[39m input_messages = _normalize_messages(messages)\n\u001B[32m    498\u001B[39m run_id = \u001B[33m\"\u001B[39m\u001B[33m-\u001B[39m\u001B[33m\"\u001B[39m.join((_LC_ID_PREFIX, \u001B[38;5;28mstr\u001B[39m(run_manager.run_id)))\n\u001B[32m--> \u001B[39m\u001B[32m499\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    500\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mid\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m:\u001B[49m\n\u001B[32m    501\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mid\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_id\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1425\u001B[39m, in \u001B[36mChatGoogleGenerativeAI._stream\u001B[39m\u001B[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001B[39m\n\u001B[32m   1399\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_stream\u001B[39m(\n\u001B[32m   1400\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1401\u001B[39m     messages: List[BaseMessage],\n\u001B[32m   (...)\u001B[39m\u001B[32m   1412\u001B[39m     **kwargs: Any,\n\u001B[32m   1413\u001B[39m ) -> Iterator[ChatGenerationChunk]:\n\u001B[32m   1414\u001B[39m     request = \u001B[38;5;28mself\u001B[39m._prepare_request(\n\u001B[32m   1415\u001B[39m         messages,\n\u001B[32m   1416\u001B[39m         stop=stop,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1423\u001B[39m         tool_choice=tool_choice,\n\u001B[32m   1424\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1425\u001B[39m     response: GenerateContentResponse = \u001B[43m_chat_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1426\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1427\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_method\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstream_generate_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1428\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1429\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdefault_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1430\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1432\u001B[39m     prev_usage_metadata: UsageMetadata | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1433\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m response:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:210\u001B[39m, in \u001B[36m_chat_with_retry\u001B[39m\u001B[34m(generation_method, **kwargs)\u001B[39m\n\u001B[32m    207\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    208\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m--> \u001B[39m\u001B[32m210\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_chat_with_retry\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:338\u001B[39m, in \u001B[36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[39m\u001B[34m(*args, **kw)\u001B[39m\n\u001B[32m    336\u001B[39m copy = \u001B[38;5;28mself\u001B[39m.copy()\n\u001B[32m    337\u001B[39m wrapped_f.statistics = copy.statistics  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m338\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001B[39m, in \u001B[36mRetrying.__call__\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    475\u001B[39m retry_state = RetryCallState(retry_object=\u001B[38;5;28mself\u001B[39m, fn=fn, args=args, kwargs=kwargs)\n\u001B[32m    476\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m477\u001B[39m     do = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    478\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[32m    479\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001B[39m, in \u001B[36mBaseRetrying.iter\u001B[39m\u001B[34m(self, retry_state)\u001B[39m\n\u001B[32m    376\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    377\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m action \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.iter_state.actions:\n\u001B[32m--> \u001B[39m\u001B[32m378\u001B[39m     result = \u001B[43maction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    379\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001B[39m, in \u001B[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001B[39m\u001B[34m(rs)\u001B[39m\n\u001B[32m    418\u001B[39m retry_exc = \u001B[38;5;28mself\u001B[39m.retry_error_cls(fut)\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.reraise:\n\u001B[32m--> \u001B[39m\u001B[32m420\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mretry_exc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    421\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m retry_exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mfut\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexception\u001B[39;00m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001B[39m, in \u001B[36mRetryError.reraise\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    185\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mreraise\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> t.NoReturn:\n\u001B[32m    186\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.last_attempt.failed:\n\u001B[32m--> \u001B[39m\u001B[32m187\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlast_attempt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    188\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:449\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    447\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[32m    448\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m449\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    451\u001B[39m \u001B[38;5;28mself\u001B[39m._condition.wait(timeout)\n\u001B[32m    453\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:401\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    399\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    400\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    403\u001B[39m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[32m    404\u001B[39m         \u001B[38;5;28mself\u001B[39m = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001B[39m, in \u001B[36mRetrying.__call__\u001B[39m\u001B[34m(self, fn, *args, **kwargs)\u001B[39m\n\u001B[32m    478\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[32m    479\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m480\u001B[39m         result = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    481\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[32m    482\u001B[39m         retry_state.set_exception(sys.exc_info())  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:208\u001B[39m, in \u001B[36m_chat_with_retry.<locals>._chat_with_retry\u001B[39m\u001B[34m(**kwargs)\u001B[39m\n\u001B[32m    204\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m ChatGoogleGenerativeAIError(\n\u001B[32m    205\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInvalid argument provided to Gemini: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    206\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    207\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m208\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:192\u001B[39m, in \u001B[36m_chat_with_retry.<locals>._chat_with_retry\u001B[39m\u001B[34m(**kwargs)\u001B[39m\n\u001B[32m    189\u001B[39m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[32m    190\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_chat_with_retry\u001B[39m(**kwargs: Any) -> Any:\n\u001B[32m    191\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgeneration_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    193\u001B[39m     \u001B[38;5;66;03m# Do not retry for these errors.\u001B[39;00m\n\u001B[32m    194\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m google.api_core.exceptions.FailedPrecondition \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1182\u001B[39m, in \u001B[36mGenerativeServiceClient.stream_generate_content\u001B[39m\u001B[34m(self, request, model, contents, retry, timeout, metadata)\u001B[39m\n\u001B[32m   1179\u001B[39m \u001B[38;5;28mself\u001B[39m._validate_universe_domain()\n\u001B[32m   1181\u001B[39m \u001B[38;5;66;03m# Send the request.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1182\u001B[39m response = \u001B[43mrpc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1184\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretry\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1185\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1186\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1187\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1189\u001B[39m \u001B[38;5;66;03m# Done; return the response.\u001B[39;00m\n\u001B[32m   1190\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001B[39m, in \u001B[36m_GapicCallable.__call__\u001B[39m\u001B[34m(self, timeout, retry, compression, *args, **kwargs)\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compression \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    129\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mcompression\u001B[39m\u001B[33m\"\u001B[39m] = compression\n\u001B[32m--> \u001B[39m\u001B[32m131\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001B[39m, in \u001B[36mRetry.__call__.<locals>.retry_wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    290\u001B[39m target = functools.partial(func, *args, **kwargs)\n\u001B[32m    291\u001B[39m sleep_generator = exponential_sleep_generator(\n\u001B[32m    292\u001B[39m     \u001B[38;5;28mself\u001B[39m._initial, \u001B[38;5;28mself\u001B[39m._maximum, multiplier=\u001B[38;5;28mself\u001B[39m._multiplier\n\u001B[32m    293\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m294\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mretry_target\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    295\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_predicate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    297\u001B[39m \u001B[43m    \u001B[49m\u001B[43msleep_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    299\u001B[39m \u001B[43m    \u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m=\u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    300\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001B[39m, in \u001B[36mretry_target\u001B[39m\u001B[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[32m    153\u001B[39m \u001B[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001B[39;00m\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    155\u001B[39m     \u001B[38;5;66;03m# defer to shared logic for handling errors\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m156\u001B[39m     next_sleep = \u001B[43m_retry_error_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    157\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    158\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdeadline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[43m        \u001B[49m\u001B[43msleep_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m        \u001B[49m\u001B[43merror_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpredicate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    162\u001B[39m \u001B[43m        \u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexception_factory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    164\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    166\u001B[39m     \u001B[38;5;66;03m# if exception not raised, sleep before next attempt\u001B[39;00m\n\u001B[32m    167\u001B[39m     time.sleep(next_sleep)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001B[39m, in \u001B[36m_retry_error_helper\u001B[39m\u001B[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001B[39m\n\u001B[32m    208\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m predicate_fn(exc):\n\u001B[32m    209\u001B[39m     final_exc, source_exc = exc_factory_fn(\n\u001B[32m    210\u001B[39m         error_list,\n\u001B[32m    211\u001B[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001B[32m    212\u001B[39m         original_timeout,\n\u001B[32m    213\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m214\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m final_exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msource_exc\u001B[39;00m\n\u001B[32m    215\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m on_error_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    216\u001B[39m     on_error_fn(exc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001B[39m, in \u001B[36mretry_target\u001B[39m\u001B[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    146\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m         result = \u001B[43mtarget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m inspect.isawaitable(result):\n\u001B[32m    149\u001B[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001B[39m, in \u001B[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    126\u001B[39m         remaining_timeout = \u001B[38;5;28mself\u001B[39m._timeout\n\u001B[32m    128\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mtimeout\u001B[39m\u001B[33m\"\u001B[39m] = remaining_timeout\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GEN-AI\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:174\u001B[39m, in \u001B[36m_wrap_stream_errors.<locals>.error_remapped_callable\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    170\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _StreamingResponseIterator(\n\u001B[32m    171\u001B[39m         result, prefetch_first_result=prefetch_first\n\u001B[32m    172\u001B[39m     )\n\u001B[32m    173\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m grpc.RpcError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m--> \u001B[39m\u001B[32m174\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exceptions.from_grpc_error(exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n",
      "\u001B[31mResourceExhausted\u001B[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-1.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 50\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 44\n}\n]"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:17.995903900Z",
     "start_time": "2025-07-05T19:51:04.633300Z"
    }
   },
   "cell_type": "code",
   "source": "agent_executor.invoke({\"input\":\"tell me something about langsmith\"})",
   "id": "2ccdf4ddc1300aaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'langsmith'}`\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[38;5;200m\u001B[1;3mLangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "\n",
      "Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
      "\n",
      "Get started by creating your first evaluation.\n",
      "Quickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\n",
      "Analyze results of evaluations in the LangSmith UI and compare results over time.\n",
      "Easily collect human feedback on your data to improve your application.\n",
      "\n",
      "ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\u001B[0m\u001B[32;1m\u001B[1;3mLangSmith is a platform designed for building and managing production-grade large language model (LLM) applications.  It offers tools to monitor and evaluate application performance, enabling quicker and more confident deployments.  Key features include evaluation tools (both automated and with human feedback), observability features for analyzing application traces and setting up metrics/alerts, and prompt engineering capabilities with version control and collaboration.\n",
      "\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me something about langsmith',\n",
       " 'output': 'LangSmith is a platform designed for building and managing production-grade large language model (LLM) applications.  It offers tools to monitor and evaluate application performance, enabling quicker and more confident deployments.  Key features include evaluation tools (both automated and with human feedback), observability features for analyzing application traces and setting up metrics/alerts, and prompt engineering capabilities with version control and collaboration.\\n'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:11:17.997902100Z",
     "start_time": "2025-07-05T19:51:07.276425Z"
    }
   },
   "cell_type": "code",
   "source": "agent_executor.invoke({\"input\": \"What's the paper 1706.03762 about?\"})",
   "id": "16eec59a33ef432c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[36;1m\u001B[1;3mPublished: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on c\u001B[0m\u001B[32;1m\u001B[1;3mThe paper with the arXiv ID 1706.03762 is titled \"Attention is All You Need\".  It was published in 2017 and authored by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.  The abstract indicates it discusses sequence transduction models.  Unfortunately, the response is truncated, so I can't give you a more detailed summary.\n",
      "\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the paper 1706.03762 about?\",\n",
       " 'output': 'The paper with the arXiv ID 1706.03762 is titled \"Attention is All You Need\".  It was published in 2017 and authored by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.  The abstract indicates it discusses sequence transduction models.  Unfortunately, the response is truncated, so I can\\'t give you a more detailed summary.\\n'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 112
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
