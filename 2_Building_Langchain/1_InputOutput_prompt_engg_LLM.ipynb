{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Getting started With Langchain And GEMINI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ],
   "id": "841b213fee5ecb1e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.question_answering.map_rerank_prompt import output_parser\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GEMINI_KEY\")\n",
    "#Langchain Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llm"
   ],
   "id": "f547eb8891a90496",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result=llm.invoke(\"give a brief introduction about generative ai \")\n",
    "result"
   ],
   "id": "ae15a4e2daadc92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### chat prompt template\n",
    "### input will firstly pass through the prompt then to llm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Suppose you are an AI engineer. Provide me answers based on the questions\"),(\"user\",\"{input}\")\n",
    "])\n",
    "prompt"
   ],
   "id": "358946600d5827c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chain=prompt|llm\n",
    "response=chain.invoke({\"input\":\"Can you tell me about github and langchain and langserve\"})\n",
    "print(response)"
   ],
   "id": "d6e0c96f92ee5284",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(response)",
   "id": "374d38d9ff1e56d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "output parser is used to show display as per our needs\n",
    "\n",
    "suppose i just want to see the output of my input no token details and any extra details except my llm response to a user then"
   ],
   "id": "3f366a297c3deaf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## stroutput parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "response=chain.invoke({\"input\":\"i am happy with my work today thanks and good night see you tommorow\"})\n",
    "print(response)"
   ],
   "id": "ebe50ed5e48c6157",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "app.py is a web interface of the working of this code run app.py",
   "id": "da0a76f9f71e71de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
