{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Build a Simple LLM Application with LCEL(LangChain Expression Language)\n",
    "LangChain Expression Language (LCEL) is an abstraction of some interesting Python concepts into a format that enables a \"minimalist\" code layer for building chains of LangChain components.\n",
    "\n",
    "In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\n",
    "\n",
    "After this , you'll have a high level overview of:\n",
    "\n",
    "- Using language models\n",
    "\n",
    "- Using PromptTemplates and OutputParsers\n",
    "\n",
    "- Using LangChain Expression Language (LCEL) to chain components together\n",
    "\n",
    "- Debugging and tracing your application using LangSmith\n",
    "\n",
    "- Deploying your application with LangServe"
   ],
   "id": "c274f6a9172ab8e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GEMINI_KEY\")\n",
    "#Langchain Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_KEY\")"
   ],
   "id": "e1560565170ee5ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "#model=ChatGroq=(model=\"gemma2-9b-it\")\n",
    "model"
   ],
   "id": "d4c8293a9f9c76c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use messages if you're sending a fixed list of instructions manually.\n",
    "\n",
    "Use ChatPromptTemplate when you want reusable, dynamic prompting with variables.\n",
    "\n",
    "to_messages() gives you the actual message objects generated from your template."
   ],
   "id": "78959d2755bdba5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "messages=[\n",
    "    SystemMessage(content=\"Translate the following from English To Hindi and provide alternative also:\"),\n",
    "    HumanMessage(content=\"Hii ,How are you ? I am here to answer all the queries that you have put in your email !! May i know in which mode of communcation you would like to communicate with us. we prefer communication over call coz we would be able to understand your queries in a better way,thanks\")\n",
    "]\n",
    "result=model.invoke(messages)\n",
    "result"
   ],
   "id": "2936a668706ec3ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Remember every model has it's on way to decide whether the whole output produced will lie in content or not\n",
    "\n",
    "parser always return result.content i.e. content of ai message\n",
    "\n",
    "so when we use groq model it will not give whole output inside content\n",
    "and gemini model put everything inside conent"
   ],
   "id": "2f2d702fe33c042a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser=StrOutputParser()\n",
    "parser.invoke(result)"
   ],
   "id": "79b5224871966d76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using LCEL to chain the components\n",
   "id": "80695331d9d96c83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chain=model|parser\n",
    "chain.invoke(messages)"
   ],
   "id": "52ac8594a75aa24c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "generic_template=\"translate the following into {language}:\"\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",generic_template),(\"user\",\"{text}\")]\n",
    ")"
   ],
   "id": "5232b7383e5e313b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result=prompt.invoke({\"language\":\"Hindi\",\"text\":\"hi ,Sachin Goyal this side ,it's interesting to learn and build gen ai projects\"})\n",
    "result.to_messages()"
   ],
   "id": "c6b3674952d52101",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Chaining together using Lcel",
   "id": "20f1b1c78ab972bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chain=prompt|model|parser\n",
    "chain.invoke({\"language\":\"Hindi\",\"text\":\"hi ,Sachin Goyal this side ,it's interesting to learn and build gen ai projects\"})"
   ],
   "id": "6e7346423a22a050",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
