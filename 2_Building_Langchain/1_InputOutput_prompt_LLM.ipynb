{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Getting started With Langchain And GEMINI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ],
   "id": "841b213fee5ecb1e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-27T20:30:29.949911Z",
     "start_time": "2025-06-27T20:30:29.898990Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.question_answering.map_rerank_prompt import output_parser\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GEMINI_KEY\")\n",
    "#Langchain Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T20:30:32.284965Z",
     "start_time": "2025-06-27T20:30:29.968384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llm"
   ],
   "id": "f547eb8891a90496",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001B060251FD0>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T20:30:35.170393Z",
     "start_time": "2025-06-27T20:30:32.303090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result=llm.invoke(\"give a brief introduction about generative ai \")\n",
    "result"
   ],
   "id": "ae15a4e2daadc92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Generative AI is a type of artificial intelligence that can create new content, rather than just analyzing or classifying existing data.  This content can take many forms, including text, images, audio, video, and even code.  It works by learning patterns and structures from vast datasets and then using that knowledge to generate similar but novel outputs.  Think of it as a sophisticated pattern-completion engine, capable of producing surprisingly creative and realistic results.  Examples include writing stories, composing music, generating realistic images, and creating software code.  While still developing, generative AI is rapidly transforming many industries and aspects of daily life.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--b867c936-e7f7-4d45-b590-dcb1274f1ad5-0', usage_metadata={'input_tokens': 8, 'output_tokens': 126, 'total_tokens': 134, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T20:30:35.197189Z",
     "start_time": "2025-06-27T20:30:35.189762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### chat prompt template\n",
    "### input will firstly pass through the prompt then to llm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Suppose you are an AI engineer. Provide me answers based on the questions\"),(\"user\",\"{input}\")\n",
    "])\n",
    "prompt"
   ],
   "id": "358946600d5827c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Suppose you are an AI engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T20:30:42.358304Z",
     "start_time": "2025-06-27T20:30:35.259451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain=prompt|llm\n",
    "response=chain.invoke({\"input\":\"Can you tell me about github and langchain and langserve\"})\n",
    "print(response)"
   ],
   "id": "d6e0c96f92ee5284",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Let\\'s break down GitHub, LangChain, and LangServe individually:\\n\\n**1. GitHub:**\\n\\n* **What it is:** GitHub is a web-based platform primarily used for software development and version control.  It\\'s a central repository where developers can store their code, collaborate on projects, and track changes over time using Git, a distributed version control system.\\n* **Key Features:**\\n    * **Version Control:**  Tracks every change made to the code, allowing developers to revert to previous versions if necessary and collaborate seamlessly.\\n    * **Collaboration:**  Multiple developers can work on the same project simultaneously, merging their changes and resolving conflicts.\\n    * **Issue Tracking:**  Allows for managing bugs, feature requests, and other tasks related to the project.\\n    * **Pull Requests:**  A mechanism for proposing changes to a project\\'s codebase, facilitating code reviews and ensuring code quality.\\n    * **Hosting:** GitHub provides hosting for project websites (GitHub Pages) and allows for automated workflows (GitHub Actions).\\n    * **Community:** A massive community of developers, making it easy to find open-source projects, contribute to them, and learn from others.\\n\\n**2. LangChain:**\\n\\n* **What it is:** LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the process of building applications that interact with LLMs, providing tools and abstractions to manage prompts, chain multiple calls to LLMs, and integrate with external data sources.\\n* **Key Features:**\\n    * **Modular Design:**  Allows easy integration of different LLMs, prompts, memory mechanisms, and other components.\\n    * **Chains:**  Facilitates the creation of sequences of operations involving LLMs, allowing for more complex applications beyond simple single-prompt interactions.  This might involve summarizing text, then translating it, then answering questions about it â€“ all in a single, chained process.\\n    * **Indexes:**  Provides tools for connecting LLMs to external data sources, enabling the models to access and process information beyond their training data.  This is crucial for applications requiring up-to-date or specific information.\\n    * **Agents:**  Allows LLMs to act autonomously, choosing which tools or actions to take based on the given task.  This enables more sophisticated and adaptive applications.\\n    * **Memory:**  Provides mechanisms for LLMs to retain information across multiple interactions, improving context and consistency in conversations.\\n\\n**3. LangServe:**\\n\\n* **What it is:** LangServe is *not* a standalone project like GitHub or LangChain.  It\\'s more of a *concept* or a *pattern* for deploying LangChain applications.  It\\'s essentially a way to package and serve LangChain applications as APIs, making them easily accessible to other applications or users.  This often involves using tools like FastAPI (a Python web framework) to create a RESTful API that exposes the functionality of a LangChain application.\\n* **Key Aspects:**\\n    * **API-driven access:**  Users interact with the LangChain application through an API, rather than directly running the code.\\n    * **Scalability:**  LangServe architectures are designed to handle multiple concurrent requests, improving the scalability of LangChain applications.\\n    * **Deployment:**  LangServe facilitates deploying LangChain applications to cloud platforms like AWS, Google Cloud, or Azure.\\n    * **No single implementation:**  There\\'s no single \"LangServe\" project; it\\'s a design pattern for deploying LangChain applications, and the specific implementation will vary depending on the chosen tools and infrastructure.\\n\\n\\nIn short: GitHub is a platform for code collaboration, LangChain is a framework for building LLM applications, and LangServe is a deployment pattern for making those applications accessible via APIs.  They are related but distinct entities.  You might use GitHub to host the code for a LangChain application, and then use LangServe principles to deploy that application as an API.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--ebdaf842-54a5-4110-8cf8-c30b087e655e-0' usage_metadata={'input_tokens': 26, 'output_tokens': 824, 'total_tokens': 850, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T20:34:39.087862Z",
     "start_time": "2025-06-27T20:34:38.986807Z"
    }
   },
   "cell_type": "code",
   "source": "type(response)",
   "id": "374d38d9ff1e56d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "output parser is used to show display as per our needs\n",
    "\n",
    "suppose i just want to see the output of my input no token details and any extra details except my llm response to a user then"
   ],
   "id": "3f366a297c3deaf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T20:49:09.815489Z",
     "start_time": "2025-06-27T20:49:09.029762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## stroutput parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "response=chain.invoke({\"input\":\"i am happy with my work today thanks and good night see you tommorow\"})\n",
    "print(response)"
   ],
   "id": "ebe50ed5e48c6157",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good night!  It's great to hear you had a productive and happy day. See you tomorrow.\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
