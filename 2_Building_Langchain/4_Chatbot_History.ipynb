{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Building a Chatbot with history",
   "id": "14fa86abc76304f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GEMINI_KEY\")\n",
    "#Langchain Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_KEY\")"
   ],
   "id": "6fd908b3d034b7a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model=\"gemma2-9b-it\")\n",
    "llm"
   ],
   "id": "efc1d6d83e26556f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "response=llm.invoke([HumanMessage(content=\"Hii, My name is Sachin Goyal.I am currently pursuing my B.Tech(3rd year) from Thapar University in CSE,i am currently practising Generative Ai\")])\n",
    "response.content"
   ],
   "id": "9bf190b6b86af641",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hii, My name is Sachin Goyal.I am currently pursuing my B.Tech(3rd year) from Thapar University in CSE,i am currently practising Generative Ai.what will you suggest me for a better learning?\") ,\n",
    "        AIMessage(content=\"Hi Sachin,\\n\\nIt's great to meet you!  It's impressive that you're already practicing Generative AI during your B.Tech in CSE at Thapar University.  That's a very forward-looking skillset to be developing.  What specific areas of Generative AI are you focusing on right now?  I'd be interested to hear more about your projects.\") ,\n",
    "        HumanMessage(content=\"What is my name?\")\n",
    "    ]\n",
    ")\n"
   ],
   "id": "39d4e341599a6db5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={} #storing all the chat with their id to uniquely define and prevent chat mixing\n",
    "def get_session_history(session_id)->BaseChatMessageHistory:\n",
    "    if session_id not in store:  #if new chat then store the chat going on\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "with_message_history=RunnableWithMessageHistory(llm,get_session_history)"
   ],
   "id": "f553e900ae2eb5e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#declaring session id chat1 for this chat for a user\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "\n",
    "response=with_message_history.invoke (\n",
    "    [ HumanMessage(content=\"Hii, My name is Sachin Goyal.I am currently pursuing my B.Tech(3rd year) from Thapar University in CSE,i am currently practising Generative Ai.what will you suggest me for a better learning?\") ]\n",
    "    ,config=config )\n",
    "response"
   ],
   "id": "eb3647b7e83c0cba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#see this time it says i don't know about you name\n",
    "#basically the session id is a new one so no previous chat available it will be stored for the first time\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my name\")],config=config\n",
    ")\n",
    "response"
   ],
   "id": "444ad6763fc803b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"ok tell me what is my name\")],config=config\n",
    ")\n",
    "response\n",
    "#see it remembers my name"
   ],
   "id": "11bc2d67546fc965",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke([HumanMessage(content=\"My name is John\")],config=config)\n",
    "response"
   ],
   "id": "1fc4d4421a8fe1ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my name\")],config=config\n",
    ")\n",
    "response\n",
    "#this time remembers the name"
   ],
   "id": "e963eac8f340490e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\n",
    "\n",
    "\n",
    "HOW IT WORKS\n",
    "\n",
    "-->we create a prompt where system command is there to llm\n",
    "\n",
    "-->we create a variable of MessagePlaceholder which will place all the previous msg+new one\n",
    "\n",
    "-->we create a chain of prompt and llm\n",
    "\n",
    "-->for chat history we have Runnable with msgHistory which gets session id and\n",
    "    retrieve all the prev msg history and provide it to chain\n",
    "\n",
    "-->the llm will generate the response based on the prev history"
   ],
   "id": "a9058ecd2efa6306"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "     [\n",
    "         (\"system\",\"You are a helpful assistant ,communicate with the user according\"),\n",
    "         MessagesPlaceholder(variable_name=\"messages\")\n",
    "     ]\n",
    " )\n",
    "chain=prompt|llm"
   ],
   "id": "58a08cfe4b5b62ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chain.invoke({\"messages\":[HumanMessage(content=\"Hii ,My name is Sachin Goyal\")]})",
   "id": "966015b1490ad42a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "with_message_history=RunnableWithMessageHistory(chain,get_session_history)",
   "id": "c056b993b7602bac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {'message':[HumanMessage(content=\"My name is Sachin Goyal\")]},config=config\n",
    ")\n",
    "response"
   ],
   "id": "820708e8c6e9de60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {'message':[HumanMessage(content=\"can you tell me what is my name,age and my fav hobby\")]},config=config\n",
    ")\n",
    "response"
   ],
   "id": "28b6a5cfc84f1ea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "What if we have multiple inputs to be taken in prompt\n",
    "--> then there is a change in with_message_history\n",
    "    we need to define a correct key to use to save chat to history"
   ],
   "id": "4007295494d07960"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "     [\n",
    "         (\"system\",\"You are a helpful assistant ,communicate with the user according in {language}\"),\n",
    "         MessagesPlaceholder(variable_name=\"messages\")\n",
    "     ]\n",
    " )\n",
    "chain=prompt|llm"
   ],
   "id": "15d1ce71bfe7fd48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chain.invoke({\"messages\":[HumanMessage(content=\"Hii ,My name is Sachin Goyal\")],\"language\":\"Hindi\"})",
   "id": "db736d7109c1b121",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "with_message_history=RunnableWithMessageHistory(chain,get_session_history,input_messages_key=\"messages\")",
   "id": "2ac3141b7d665bc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "whenever we reuse the older session for ex chat1 then it loads the old history and as we used english by default so now declaring any other language may not work for that and will print in english",
   "id": "d66878c48bff0549"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"can you tell me my name \")],\"language\":\"Hindi\"},config=config\n",
    ")\n",
    "response"
   ],
   "id": "afef132f1e4c31e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat5\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"My name is Sachin Goyal\")],\"language\":\"Hindi\"},config=config\n",
    ")\n",
    "response"
   ],
   "id": "996adf0a8342f763",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat5\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"can you tell me my name \")],\"language\":\"Hindi\"},config=config\n",
    ")\n",
    "response"
   ],
   "id": "bf5016f7dd752ae4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ],
   "id": "d91b0035c94c2dbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy='last', # keep the msgs from last with tokens<=max_tokens\n",
    "    token_counter=llm,\n",
    "    allow_partial=False, #If a msg pushes total tokens over 45, it wonâ€™t partially include it instead, it will stop adding messages.\n",
    "    include_system=True, # as system guides the model what should it do so it must be present anyhow even after trimming msgs\n",
    "    start_on='human'  #look for the most recent HumanMessage and start including from there.\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ],
   "id": "bfd75dc6c1b11809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "maybe the trimmer trimmed the ice cream part here so not working check next code section",
   "id": "ddd23d7b6dbea649"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain=(\n",
    "        RunnablePassthrough.assign(messages=itemgetter(\"messages\") |trimmer)\n",
    "       | prompt\n",
    "       | llm\n",
    ")\n",
    "response=chain.invoke(\n",
    "    {\"messages\":messages+[HumanMessage(content=\"what ice cream do i like\")],\"language\":\"Hindi\"}\n",
    ")\n",
    "response"
   ],
   "id": "ad48d85361c6942d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain=(\n",
    "        RunnablePassthrough.assign(messages=itemgetter(\"messages\") |trimmer)\n",
    "       | prompt\n",
    "       | llm\n",
    ")\n",
    "response=chain.invoke(\n",
    "    {\"messages\":messages+[HumanMessage(content=\"what was my question regarding maths\")],\"language\":\"Hindi\"}\n",
    ")\n",
    "response"
   ],
   "id": "d65a7b8d2157dfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wrapping with Message history (summary only)\n",
   "id": "89bcb3d1f05ae272"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "with_message_history=RunnableWithMessageHistory(chain,get_session_history,input_messages_key=\"messages\")",
   "id": "11ded0d3835f1412",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat6\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":messages+[HumanMessage(content=\"what is my name\")],\"language\":\"Hindi\"},config=config\n",
    ")\n",
    "response"
   ],
   "id": "7c80d5d8804c3eef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat6\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":messages+[HumanMessage(content=\"My name is Sachin,can you tell me the meaning of this \")],\"language\":\"Hindi\"},config=config\n",
    ")\n",
    "response"
   ],
   "id": "727940c11440c2b6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
