{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Doc chain to generate answer from llm based on provided context and query asked\n",
    "Retrieval chain for getting answer by comparing closeness of vector from the stored data (KNN concept)\n",
    "\n",
    "Ret+Doc chain concept see below for all 3 concepts of doc,ret,doc+ret\n",
    "\n",
    "Complete Basic RAG flow"
   ],
   "id": "5c217ade9f264054"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GEMINI_KEY\")\n",
    "#Langchain Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ],
   "id": "c5594c9fe42cfec3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Data--> Docs-->Divide our Docuemnts into chunks dcouments-->text-->vectors-->Vector Embeddings--->Vector Store DB",
   "id": "b5215be52962d600"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#data ingestion\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/administration/tutorials/manage_spend\")\n",
    "loader"
   ],
   "id": "62d2630ecc2b4b06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "docs=loader.load()\n",
    "docs"
   ],
   "id": "94cb6c672e95a163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#data transformation\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "splitted=splitter.split_documents(docs)\n",
    "splitted"
   ],
   "id": "c5f23048778d7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "embeddings"
   ],
   "id": "d1d7eeb2bc2df1dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "db=FAISS.from_documents(splitted,embeddings)\n",
    "db"
   ],
   "id": "61665c354dda42fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#asking for an answer to the query\n",
    "query=\"LangSmith has two usage limits: total traces and extended\"\n",
    "result=db.similarity_search(query)\n",
    "result[1].page_content"
   ],
   "id": "a6b99086d44f9e8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "llm=GoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llm"
   ],
   "id": "33a1bb14045e4702",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "now we saw that in prompt we can design our llm to act as a specific \" engg or a thing\" while answering the queries\n",
    "now to get output from a specific content we can use document chain"
   ],
   "id": "9fc51cedc007a2e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ],
   "id": "74f1d10c3cfdc6e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"LangSmith has two usage limits: total traces and extended\",\n",
    "    \"context\":[Document(page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph. and it can help us to track our billings also\")]\n",
    "})"
   ],
   "id": "2c10484ce2f27072",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "now we gave our own context and asked a query based on it the llm answered this is our document_chain\n",
    "and\n",
    "earlier we used retreiver to get answer of the query from the stored data\n",
    "\n",
    " Q : What if we want a llm to generate an answer for the query from the stored data??\n",
    "\n",
    "     this needs to used retriever to get relevant data using vector close distance\n",
    "     then document chain to get all that data and generate an answer that relates to query\n",
    "\n",
    "we asked query--->goes to retriever ->>> search in vectore store first--> get all the relevant data ->feed the doc chain with that output data  and question  to get the output"
   ],
   "id": "e40e9128518eda35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "db",
   "id": "d6b9ccae3c46bea4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "retriever=db.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain"
   ],
   "id": "2e3735bdeeff782f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#getting the response from the llm\n",
    "response=retrieval_chain.invoke({\"input\":\"LangSmith has two usage limits: total traces and extended\"})\n",
    "response['answer']"
   ],
   "id": "36743330e97538d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "response['context']",
   "id": "778398c333b6b521",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#provides context +answer both\n",
    "response"
   ],
   "id": "c42d0cf3594b2be4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
