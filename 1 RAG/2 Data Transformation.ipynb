{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now splitting the data into chunks so that if there is large data then not to cross the limits",
   "id": "d2645b17da294900"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader('speech.txt')\n",
    "docs=loader.load()\n",
    "print(docs)\n",
    "print(f\"type of docs is {type(docs)}\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "remember docs is a list of documents\n",
    "so if we use docs[0] it same as docs  and it will throw error for docs[1] as docs is a list of all the documents\n",
    "so nothing present at index1\n",
    "now after splitting\n",
    "splitted_docs is a list of splitted docs\n",
    "and at every index we have docs so splitted_docs[1]....[n] will also work"
   ],
   "id": "f6ba322fb2169ad2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "now splitting the data\n",
    "\n",
    "as already mentioned after loading  speech.txt and then loading that it become a list of documents\n",
    "\n",
    "so if we use text_splitter.create_documents()\n",
    "\n",
    "text splitter creates seperate documents of the lists of documents at seperate indexes\n",
    "\n",
    "but here it's not possible as it already a list of documents\n",
    "so now we will use split_documents\n",
    "it will split all the list of documents into seperate documents\n",
    "\n",
    "chunk size means for every document how many characters and overlap means how many characters from previous document will be included in next document"
   ],
   "id": "2972ab870781323a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=30,chunk_overlap=11)\n",
    "splitted_doc=text_splitter.split_documents(docs)\n",
    "print(splitted_doc[0])\n",
    "print(splitted_doc[1])\n",
    "type(splitted_doc[1])"
   ],
   "id": "365f095113342a04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"type of splitted_doc is {type(splitted_doc)}\")\n",
    "print(splitted_doc)"
   ],
   "id": "618b879b841b46d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for doc in splitted_doc:\n",
    "    print(doc,end='\\n\\n')"
   ],
   "id": "79c4d7b12c0a75b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " SPLITTING THE PDF\n",
    "\n",
    "same concept here\n"
   ],
   "id": "4d7f96b3ce32e576"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "load_document=PyPDFLoader('1_Intro to AI.pdf')\n",
    "pdf_doc=load_document.load()\n",
    "print(pdf_doc)"
   ],
   "id": "70566cd2a8653881",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=10)\n",
    "splitted_doc=text_splitter.split_documents(pdf_doc)\n",
    "print(splitted_doc[0])\n",
    "print(splitted_doc[1])\n",
    "print(f\"type of splitted_doc is {type(splitted_doc[0])}\")"
   ],
   "id": "ee1c2743ee42de0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"type of splitted_doc is {type(splitted_doc)}\")\n",
    "print(splitted_doc)"
   ],
   "id": "a80afc745d700d94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for doc in splitted_doc:\n",
    "    print(doc,end='\\n\\n')"
   ],
   "id": "c20fe632368a5fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "NOW CREATING DOCS\n",
    "\n",
    "earlier we were having a list of documents so unable to create documents\n",
    "what if we have a string\n",
    "then we can create doc from it"
   ],
   "id": "2933775ff12320a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "speech=\"\"\n",
    "with open('speech.txt','r') as f:\n",
    "    speech=f.read()\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=30)\n",
    "splitted_doc=text_splitter.create_documents([speech])  #as textsplitt splits list of docs so used [speech]\n",
    "print(splitted_doc[0])\n",
    "print(splitted_doc[1])"
   ],
   "id": "e010be27de7b45ff",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
