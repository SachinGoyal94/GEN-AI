{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now splitting the data into chunks so that if there is large data then not to cross the limits",
   "id": "d2645b17da294900"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:21:56.174172Z",
     "start_time": "2025-06-27T12:21:56.168137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader('speech.txt')\n",
    "docs=loader.load()\n",
    "print(docs)\n",
    "print(f\"type of docs is {type(docs)}\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'speech.txt'}, page_content='Hi this is a content generated by me\\nfor the demonstration of document loaders for langchain community\\nwelcoming you\\ncheck documentation for the more type of document loaders\\nRAG means Retrieval Augumented Generation\\nIt is a important part of Gen Ai\\nGen Ai is a trend field now a days\\nIt is important to understand basics of Gen Ai\\nPython is must to do\\nAll the best')]\n",
      "type of docs is <class 'list'>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "remember docs is a list of documents\n",
    "so if we use docs[0] it same as docs  and it will throw error for docs[1] as docs is a list of all the documents\n",
    "so nothing present at index1\n",
    "\n",
    "now after splitting ,\n",
    "splitted_docs is a list of splitted docs\n",
    "\n",
    "and at every index we have docs so splitted_docs[1]....[n] will also work"
   ],
   "id": "f6ba322fb2169ad2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "now splitting the data\n",
    "\n",
    "as already mentioned after loading  speech.txt and then loading that it become a list of documents\n",
    "\n",
    "so if we use text_splitter.create_documents()\n",
    "\n",
    "text splitter creates seperate documents of the lists of documents at seperate indexes\n",
    "\n",
    "but here it's not possible as it already a list of documents\n",
    "so now we will use split_documents\n",
    "it will split all the list of documents into seperate documents\n",
    "\n",
    "chunk size means for every document how many characters and overlap means how many characters from previous document will be included in next document"
   ],
   "id": "2972ab870781323a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:24:27.672847Z",
     "start_time": "2025-06-27T12:24:27.198910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=30,chunk_overlap=11)\n",
    "splitted_doc=text_splitter.split_documents(docs)\n",
    "print(splitted_doc[0])\n",
    "print(splitted_doc[1])\n",
    "type(splitted_doc[1])"
   ],
   "id": "365f095113342a04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Hi this is a content generated' metadata={'source': 'speech.txt'}\n",
      "page_content='generated by me' metadata={'source': 'speech.txt'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T12:24:28.813641Z",
     "start_time": "2025-06-27T12:24:28.809116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"type of splitted_doc is {type(splitted_doc)}\")\n",
    "print(splitted_doc)"
   ],
   "id": "618b879b841b46d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of splitted_doc is <class 'list'>\n",
      "[Document(metadata={'source': 'speech.txt'}, page_content='Hi this is a content generated'), Document(metadata={'source': 'speech.txt'}, page_content='generated by me'), Document(metadata={'source': 'speech.txt'}, page_content='for the demonstration of'), Document(metadata={'source': 'speech.txt'}, page_content='of document loaders for'), Document(metadata={'source': 'speech.txt'}, page_content='for langchain community'), Document(metadata={'source': 'speech.txt'}, page_content='welcoming you'), Document(metadata={'source': 'speech.txt'}, page_content='check documentation for the'), Document(metadata={'source': 'speech.txt'}, page_content='for the more type of document'), Document(metadata={'source': 'speech.txt'}, page_content='document loaders'), Document(metadata={'source': 'speech.txt'}, page_content='RAG means Retrieval'), Document(metadata={'source': 'speech.txt'}, page_content='Retrieval Augumented'), Document(metadata={'source': 'speech.txt'}, page_content='Augumented Generation'), Document(metadata={'source': 'speech.txt'}, page_content='It is a important part of Gen'), Document(metadata={'source': 'speech.txt'}, page_content='of Gen Ai'), Document(metadata={'source': 'speech.txt'}, page_content='Gen Ai is a trend field now a'), Document(metadata={'source': 'speech.txt'}, page_content='now a days'), Document(metadata={'source': 'speech.txt'}, page_content='It is important to understand'), Document(metadata={'source': 'speech.txt'}, page_content='understand basics of Gen Ai'), Document(metadata={'source': 'speech.txt'}, page_content='Python is must to do'), Document(metadata={'source': 'speech.txt'}, page_content='All the best')]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for doc in splitted_doc:\n",
    "    print(doc,end='\\n\\n')"
   ],
   "id": "79c4d7b12c0a75b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " SPLITTING THE PDF\n",
    "\n",
    "same concept here\n"
   ],
   "id": "4d7f96b3ce32e576"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "load_document=PyPDFLoader('1_Intro to AI.pdf')\n",
    "pdf_doc=load_document.load()\n",
    "print(pdf_doc)"
   ],
   "id": "70566cd2a8653881",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=10)\n",
    "splitted_doc=text_splitter.split_documents(pdf_doc)\n",
    "print(splitted_doc[0])\n",
    "print(splitted_doc[1])\n",
    "print(f\"type of splitted_doc is {type(splitted_doc[0])}\")"
   ],
   "id": "ee1c2743ee42de0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"type of splitted_doc is {type(splitted_doc)}\")\n",
    "print(splitted_doc)"
   ],
   "id": "a80afc745d700d94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for doc in splitted_doc:\n",
    "    print(doc,end='\\n\\n')"
   ],
   "id": "c20fe632368a5fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "NOW CREATING DOCS\n",
    "\n",
    "earlier we were having a list of documents so unable to create documents\n",
    "what if we have a string\n",
    "then we can create doc from it"
   ],
   "id": "2933775ff12320a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "speech=\"\"\n",
    "with open('speech.txt','r') as f:\n",
    "    speech=f.read()\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=30)\n",
    "splitted_doc=text_splitter.create_documents([speech])  #as textsplitt splits list of docs so used [speech]\n",
    "print(splitted_doc[0])\n",
    "print(splitted_doc[1])"
   ],
   "id": "e010be27de7b45ff",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
